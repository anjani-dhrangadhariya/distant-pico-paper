\documentclass[10.7pt,]{article}

\usepackage[letterpaper, margin=2.54cm, top=2.54cm]{geometry}
\usepackage[super,comma,sort&compress]{natbib}
\usepackage{lmodern}
\usepackage{authblk} % To add affiliations to authors
\usepackage{amssymb,amsmath}
\usepackage{wrapfig}
\usepackage{graphicx,grffile}
\usepackage[labelfont=bf,labelsep=period]{caption}
\usepackage{ifxetex,ifluatex}
\usepackage{bm}
\usepackage{array, boldline, makecell, booktabs}
\usepackage{multirow}


%\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
    \setmainfont[]{Arial Narrow}
    \setsansfont[]{Century Gothic}
    \setmonofont[Mapping=tex-ansi]{Consolas}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
	\usepackage{microtype}
	\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}

\usepackage{lipsum} % for dummy text only REMOVE

\newtheorem{exm}{Example}


%==============================
% Customization to make the output PDF 
% look similar to the MS Word version
%==============================
% To prevent hyphenation
\hyphenpenalty=10000
\exhyphenpenalty=10000

% To set the sections font size
\usepackage{sectsty}
\allsectionsfont{\fontsize{11}{11}\selectfont}
\sectionfont{\fontsize{14}{14}\selectfont}
\subsectionfont{\bfseries\fontsize{13}{13}\selectfont}
\subsubsectionfont{\bfseries\fontsize{11}{11}\selectfont}
%\subsubsectionfont{\normalfont}

% Spacing
\usepackage{setspace}

\usepackage{xcolor}

% No new line after subsubsection
\makeatletter
%\renewcommand\subsubsection{\@startsection{subsubsection}{3}{\z@}%
%	{-3.25ex\@plus -1ex \@minus -.2ex}%
%    {-1.5ex \@plus -.2ex}% Formerly 1.5ex \@plus .2ex
%    {\normalfont}}
%\makeatother

\makeatletter % Reference list option change
\renewcommand\@biblabel[1]{#1.} % from [1] to 1
\makeatother %

% To set the doc title font
\usepackage{etoolbox}
\makeatletter
\patchcmd{\@maketitle}{\LARGE}{\bfseries\fontsize{15}{16}\selectfont}{}{}
\makeatother

% No page numbering
\pagenumbering{gobble}

\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother

% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
%==============================
\usepackage{hyperref}
\hypersetup{
	unicode=true,
	pdftitle={My Cool Title Here},
	pdfauthor={Author One, Author Two, Author Three},
	pdfkeywords={keyword1, keyword2},
	pdfborder={0 0 0},
	breaklinks=true
}
\urlstyle{same}  % don't use monospace font for urls

% Keywords command
\providecommand{\keywords}[1]
{
  \small	
  \textbf{Key words---} #1
}
%==============================

% reduce space between title and beginning of page
\title{\vspace{-2em} Not So Weak-PICO: Leveraging weak supervision for Participants, Interventions, and Outcomes recognition for systematic review automation}
\date{\vspace{-5ex}}
\author[ ] {
    % Authors
    \bf\fontsize{13}{14}\selectfont
    Anjani Dhrangadhariya,\textsuperscript{\rm 1, 2}
    Henning M\"uller \textsuperscript{\rm 1, 2}
}
\affil[1]{Institute of Business Information Systems, University of Applied Sciences Western Switzerland (HES-SO Valais-Wallis), Sierre, Switzerland}
\affil[2]{Department of Computer Science, University of Geneva (UNIGE), Geneva, Switzerland}
\affil[*]{Corresponding author: Anjani Dhrangadhariya, Institute of Business Information Systems, University of Applied Sciences Western Switzerland (HES-SO Valais-Wallis), Sierre, Switzerland; anjani.dhrangadhariya@hevs.ch}
%==============================
\begin{document}
\maketitle
\vspace{2em} %separation between the affiliations and abstract
%==============================
\doublespacing
%==============================
\section{ABSTRACT}
\label{abstract}
%==============================
% Note: Abstract is limited to 250 words
%
\textbf{Objective:}
PICO analysis is vital but time-consuming for conducting systematic reviews (SR). 
Supervised machine learning can help fully automate it, but a lack of large annotated corpora restricts innovation and adoption of automated PICO recognition systems.
The largest-available PICO corpus is manually annotated, which is too expensive for the most scientific community.
Additionally, depending upon the SR question, PICO criteria are extended to PICOS (S-Study type or design), PICOC (C-Context), and PICOT (T-timeframe), meaning the static hand-labelled corpora should undergo costly re-annotation as per the downstream requirements.
We aim to test the feasibility of designing a weak supervision system to extract these entities without hand-labelled data.

\textbf{Methodology:}
We use multiple medical and non-medical ontologies and expert-generated rules to obtain labels for PICO entities.
These labels were aggregated using simple majority voting and generative modelling.
The resulting programmatic labels were used as weak signals to train a weakly-supervised discriminative model and observe performance changes.
Additionally, we explore the current PICO gold standard errors that could have led to inaccurate evaluation of several automation methods.\\

\textbf{Results:}
We present Weak-PICO+, a weakly supervised PICO entity recognition approach using medical ontologies, non-medical ontologies and various expert-generated rules and dictionaries.
Unlike manual annotation, Weak-PICO+ does not use hand-labelled data and is quickly adaptable and extensible to other entities.
We demonstrate this by extracting an additional Study Type and Design entity extending to PICOS without costly reannotation.\\

\textbf{Conclusion:}
Weak supervision using weak-PICO+ for PICO entity recognition has encouraging results and readily extends to more clinical entities.\\
%
%
%


\keywords{Weak supervision, Machine learning, Information extraction, Evidence-based Medicine}
%
\clearpage
% Full paper is limited to 4000 words (approx. 14.6 pages)
%
%
%
%==============================
\section{INTRODUCTION}\label{introduction}
%==============================
%
Systematic Review (SR) is an evidence-based practice of answering clinical questions using a transparent and quantitative approach whereby the reviewers must collect as many research publications as possible, identifying the relevant ones from these and integrating their results via statistical analysis.
Filtering relevant publications from the bulk collected ones often uses PICO (Participants, Interventions, Comparators, Outcomes) criteria. 
A clinical study or a publication is relevant only for answering a question if it studies relevant participants, interventions (and their comparators) and outcomes~\cite{uman2011systematic}. 
Manually analyzing PICO information from thousands of publications for a single SR takes about 12 months of two medical experts' time.
The process can be automated using machine learning (ML) by directly pointing the human reviewers to the correct text chunks describing PICO.
Hand labelling corpora with PICO information requires people with combined medical and informatics skills, which is expensive and time-consuming in terms of the actual annotation process and annotator training.


Moreover, labelling PICO information is tricky because of the high disagreement between human annotators on the exact spans constituting PICO, leading to human errors in hand-labelled corpora~\cite{brockmeier2019improving}.
Additionally, depending upon the systematic review question, PICO criteria are extended to PICOS (S-Study design), PICOC (C-Context), and PICOT (T-timeframe)~\cite{riva2012your,methley2014pico,uman2011systematic}.
Hand-labelled datasets are static and prohibit quick manual re-labelling in case of human errors or when a downstream task requires new entities.
%In SRs, the step after PICO information analysis is to identify the risk of bias from the studies, which is another information extraction lacking a manually labelled corpus. 
This annotation bottleneck has pivoted attention towards weakly-supervised learning that relies on programmatic labelling sources to obtain training data.
Programmatic labelling is quick and allows efficient modifications to the training data labels per the downstream application changes.


Weakly-supervised (WS) learning has demonstrated its prowess for clinical document classification and relation extraction, but clinical entity extraction tasks have heavily relied on fully supervised (FS) approaches~\cite{meng2018weakly,wang2019clinical,mintz2009distant,elangovan2020assigning,weber2020pedl,mallory2020extracting}.
Despite the availability of UMLS, a large compendium of medical ontologies, which could be re-purposed for weak entity labelling, it has not been extensively applied to clinical entity labelling~\cite{humphreys1998unified}.
Several legacy clinical applications are also supported by rule-based \textit{if-else} systems relying on keyword cues that aid weak labelling~\cite{friedlin2008software,kim2017extracting,yang2015automatic}.
With so many weak labelling sources available, the challenge for weak supervision is efficiently aggregating these sources of varying accuracy.
%Medical ontology compendium like UMLS with several vocabularies, other external vocabularies and open access knowledgebases are an excellent resource for building clinical entity labelled training data~\cite{humphreys1998unified}. 
%\textcolor{blue}{Give some paper examples where they utilize non-generative models to combine multiple labelling sources.}
%There have been some fundamental works investigating how to combine multiple label sources of variable accuracy by ranking their importance based on their probability of error and efficiently assembling them.
%The problem of aggregating several weak labellers has now transitioned to training a generative model that estimates the accuracy of each labelling source~\cite{ratner2016data,safranchik2020weakly,lison2021skweak}.
%\textcolor{blue}{Expound on weakly-supervised learning and generative modelling using Snorkel, AutoNER, Swellshark.}

Data programming is a domain agnostic generative modelling approach combining multiple weak labelling sources and estimating their accuracies.
The effectiveness of data programming for biomedical entity recognition has been explored by Fries \textit{et al.} in their Trove system.
However, Trove only explores the very defined entities like chemical, disease, disorder and drug classes~\cite{fries2021ontology}. 
PICO categories are fuzzier in comparison and much more intricate in that they can be subdivided into subclasses.
By definition, PICO categories are spans that encompass multiple sub-class entities.
A shortcoming of span extraction is that even after a machine points a human reviewer to the correct PICO span, the reviewer requires to manually read and understand its finer aspects to screen the study for relevance.
Span extraction hence leads to semi-automation but hinders full-automation.
The entity recognition approach to PICO is not as easy as the entity recognition approach to disease or chemical names which are more or less standardized.
PICO terms are not standard, and even the experts disagree on the exact tokens constituting PICO~\cite{brockmeier2019improving}.
Weakly-supervised PICO entity extraction has not garnered enough attention as supervised span extraction.
As far as our knowledge goes, only two studies exist for weakly-supervised PICO recognition.
One of these approaches only explores distant supervision for intervention extraction using a single labelling source~\cite{dhrangadhariya2022distant}, and the other relies on a single source of programmatic labelling and focuses on PICO spans and sentences rather than entities.



%Weak supervision for PICO entities has several challenges.
%The first challenge is defining the subclasses encompassed within PICO spans.
%The second challenge is mapping the several ontologies, vocabularies and terminologies to these subclasses.
The challenges to developing weak supervision approaches to PICO entity recognition are first defining the subclasses within PICO spans and then mapping several available ontologies and terminologies to these.
The next challenge is developing weakly-supervised classifiers by optimally combining several ontologies and evaluating their performance compared to full supervision.
Another challenge is developing higher-cost expert-generated rules corresponding to these subclasses to aid ontology classifiers and evaluate their combined performance.
We also identified lags in the currently available EBM-PICO training dataset and corrected them for reliable evaluation of the WS approaches.
This work demonstrates the feasibility of using weak supervision for PICO entity extraction using the EBM-PICO benchmark.
We show how using only ontology-dependent classifiers vs combining them with more expensive expert-generated rules compares to fully-supervised extraction and, in some instances overtaking it.
%
%
%
%==============================
\section{METHODOLOGY}\label{methods}
%==============================
%
The birds-eye view of our approach is shown in the Figure.
%
\begin{figure}[ht]
\centering
\includegraphics[width=0.98\textwidth]{figures/approach.pdf}
\caption{Approach overview.}
\label{fig: Weak PICO entity extraction approach}
\end{figure}
%
%
%
%==============================
\subsection{Datasets and tasks}\label{data}
%==============================
%
%\textbf{EBM-PICO.}
EBM-PICO is a widely used dataset with PICO annotations at two levels: span-level or coarse-grained and entity-level or fine-grained (see Table~\ref{tab:coarsefineconcept}).
Span-level annotations encompass the maximum information about each class.
Entity-level annotations cover the more fine-grained information at the entity level, with PICO classes further divided into fine-grained subclasses.
The dataset comes pre-divided into a training set (n=4,933) annotated through crowd-sourcing and an expert annotated gold test set (n=191) for evaluation purposes.


%\textbf{EBM-PICO corrected + Study type}
The EBM-PICO annotation guidelines caution about variable annotation quality~\footnote{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6174533/bin/NIHMS988059-supplement-Appendix.pdf}.
Low annotation quality in the training dataset is pardonable, but the errors in the test benchmark will lead to faulty evaluation of the downstream ML methods.
We evaluate 1\% of the EBM-PICO training set tokens to gauge the possible reasons for the errors and use this error evaluation exercise to conduct an error-focused corpus re-annotation for the EBM-PICO gold corpus (n = 191) for PICO entities.
%We annotated another entity designated ``Study type and design'' for this corpus to measure the effectiveness and extensibility of weak supervision approaches.
%Our ``Study type and design'' entity is restricted to the type and design of a clinical study.
%This annotation exercise demonstrated the application of weak supervision for entities beyond PICO~\cite{menard2019turning}.
These datasets are pre-tokenized and did not require additional preprocessing except the addition of POS tags and token lemma using spaCy.
Multi-class fine-grained PICO annotations were binarized, i.e. a token label was reset to 1 if the token represented a fine-grained entity.
%All the state of the art approaches use non-binarized labels for fine-grained PICO recognition making them incomparable to our approach.
%
%
%
\begin{table}[h!]
\begin{center}
\begin{tabular}{| c | c | c | c |} 
\hline
 & P & I/C & O \\ 
\hline
0 & No label & No label & No label \\ 
1 & Age & Surgical & Physical \\ 
2 & Sex & Physical & Pain \\
3 & Sample size & Drug & Mortality \\
4 & Condition & Educational & Side effect \\
5 &  & Psychological & Mental \\
6 &  & Other & Other \\
7 &  & Control &  \\
\hline
\end{tabular}
\caption{\label{tab:coarsefineconcept} P (Participant), I (Intervention) and O (Outcome) represent the coarse-grained labels which are further divided into respective fine-grained labels.}
\end{center}
\end{table}
%
%
%
%==============================
\subsection{Sequence labeling}\label{seq_lab}
%==============================
%
Automatically labelling a corpus with PICO entities is a classical binary sequence labelling problem whereby a labeller maps an input sequence of n text tokens, $ \bm{X} = (x_{1}, x_{2}, \dotso , x_{n} )$ to output sequence $\bm{Y} = (y_{1}, y_{2}, \dotso , y_{n} )$, where $y_{i} \subset y; y = \{1,0\} $ is the label for token $x_{i}$.
In weak supervision, $\bm{Y}$ is latent and should be estimated by aggregating several weak labellers of variable accuracy.
The estimates $\bm{\hat{Y}}$ of $\bm{Y}$ are assigned as probabilistic token labels of $\bm{X}$ leading to a weakly labelled dataset that can be used to train downstream ML models.
%
%
%
%==============================
\subsection{Labelling sources}\label{lss}
%==============================
%
We used the 2021AB-full release of UMLS Metathesaurus English subset dump with 223 vocabularies.
After removing non-English and zoonotic source vocabularies as well as sources containing fewer than 500 terms, we remained with 127 vocabularies~\cite{humphreys1998unified}.
Terms in the selected vocabularies were preprocessed by removing stopwords, numbers, punctuation's and were lower-cased using Smart lower casing to preserve abbreviations.
Non-UMLS vocabularies are detailed in Table~ref{tab:source2targets}~\cite{schriml2012disease,robinson2008human,he2014oae,de2010chemical,lin2020cto,kronk2020development,geifman2011towards,rogier2021using,lin2018cancer,mohammed2012building,ninot2018definition}.
%Additional vocabularies included disease ontology (DO), Human Phenotype Ontology (HPO), Ontology of adverse events (OAE), Chemical entities of biological interest (ChEBI),  comparative toxicogenomics database (CTD) Chemical and Disease subclasses, Clinical Trials Ontology (NDD-CTO), Gender, Sex, and Sexual Orientation Ontology (GSSO), Chemotherapy Toxicities Ontology (ONTOTOX), Cancer Care: Treatment Outcomes Ontology (CCTOO), symptoms ontology (SYMP), Non-pharmacological interventions ontology (NPI), Nursing Care Coordination Ontology (NCCO)~\cite{schriml2012disease,robinson2008human,he2014oae,de2010chemical,lin2020cto,kronk2020development,geifman2011towards,rogier2021using,lin2018cancer,mohammed2012building,ninot2018definition}.
Regular expressions (ReGeX) and heuristics like POS tag cues were used to capture recurring class-specific patterns otherwise not captured by standardized terminologies.
The hand-coded dictionaries were designed using the official websites listing patient reported outcome (PROMs) questionnaires~\footnote{https://www.thoracic.org/members/assemblies/assemblies/bshsr/patient-outcome/} and PROMs~\footnote{https://www.safetyandquality.gov.au/our-work/indicators-measurement-and-reporting/patient-reported-outcomes/proms-lists}.
Vocabularies are structured, standardized data sources that do not capture various writing variations from clinical literature and custom-built ReGeX are restricted by either task or even entity type~\cite{ratner2017snorkel,safranchik2020weakly}.
Therefore, we used distant supervision dictionaries created from the structured fields of clinicaltrials.gov (CTO) as described by~\cite{dhrangadhariya2022distant}.
Principal investigators of the clinical study manually enter data in CTO, thereby incorporating large-scale writing variations.  
%
%
%
%==============================
\subsection{Labeling functions}\label{lfs}
%==============================
%
In a binary token labelling task, a labelling function is a weak classifier $\lambda$ that uses domain-specific labelling sources $\bm{S}$ and a logic to emit token labels $ \widetilde{\bm{Y_{i}}}$ with labels $ \widetilde{y} \in \{-1, 0, +1\}$ for a subset of input $\bm{X_{i}}$ tokens.
A labelling function designed for a particular target class $t \in \bm{T}$ (in here; $\bm{T} \subset \{ Participant, Intervention, Outcome \} $) should output  $1$ for the positive token label, $0$ for the negative token label, and abstain ($-1$) on the tokens where decision-making is confusing $\lambda \mapsto \{-1, 0, +1\}$.
We designed three labelling functions depending on the types of labelling sources.
The ontology or dictionary labelling functions for a target class take a dictionary of terminologies mapped to one of $y \subset \{0, +1\} $ token labels.
Relevant bigram word co-occurrences were used to account for fuzzy span matching from the terminologies.
A ReGeX labelling function for a target class takes regex patterns for $\{-1, +1\}$ labels and abstains from the rest.
A heuristic labelling function is personalized for each target class and takes a generic regex pattern and specific POS (part-of-speech) tag filters.
%A distant supervision labelling function is designed to extract non-standardized terms from a knowledge base and use them for mapping tokens to $\{0, +1\}$ labels.
%As ReGeX, heuristics, hand-coded dictionaries and distant supervision were designed for individual PICO classes; they are clubbed under the task-specific labelling functions category.
Clinical studies have several abbreviations taken into account using heuristics to identify abbreviations from the training text and assign these to either of the target classes. 
Ontology labellers then use these target class specific abbreviations.
Any labelling function using ontologies or dictionaries used string matching as the labelling heuristic.
%
%
%
%==============================
\subsection{Sources to Targets}\label{s2t}
%==============================
%
UMLS concepts are organized under semantic type categories (e.g. disease, sign and symptoms, age group, etc.) so mapping these to PICO targets invariably maps the concepts to target classes. 
%Mapping the UMLS semantic type categories to PICO target classes invariably maps concepts to relevant PICO.
It is a challenging expert-led activity, though decomposing PICO into subclasses greatly helps map sources to target classes.
A semantic category was either marked $\{1, 0\}$ depending upon its relevance to the target class.
Non-UMLS vocabularies were obtained from NCBO bioportal and were chosen to be PICO target specific and were assigned to a single label.
Target-specific distant supervision dictionaries are created from the structured fields of clinicaltrials.gov (CTO). 
The structured field ``Condition or Disease'' was mapped to the Participant target, and the ``Intervention/Treatment'' field was mapped to the Intervention target.
The semi-structured ``Primary Outcome Measures'' and ``Secondary Outcome Measures'' fields are mapped to the Outcomes class.
Hand-crafted dictionaries are separately designed for participant gender, intervention comparator terms and outcomes.
%
%
%
%==============================
\subsection{LF aggregation}\label{lms}
%==============================
%
Each labelling function $ \lambda_{i} \in \bm{\Lambda^{m}}; \bm{\Lambda} = \{\lambda_{1}, \lambda_{2}, \dotso, \lambda_{m} \} $ maps inputs $\bm{X^{n}}$ to output sequence $ \widetilde{\bm{Y^{n}}}$ with labels $\widetilde{y} \in \{-1, 0, +1\}$ yielding a label matrix $ \lambda \subset \{-1, 0, +1\}^{m \times n}$.
The labelling functions can be aggregated using the majority vote (MV) rule, where a token label is elected only when a majority of $\lambda_{i}$ vote for it. 
Ties and abstains lead to the selection of the majority label.
However, MV does not take into account the variable accuracies of different labelling sources weighing them equally.
These accuracies could be estimated without manifest ground truth based on observed agreements and disagreements rates between labelling function pairs $ \lambda_{i}, \lambda_{j}$.
We used Snorkel implemented label model that estimates these LF accuracies using a generative model $ \bm{P} ( \Lambda , Y )$ which could be ultimately used as token label probablities $\bm{\hat{Y}}$ for label classes $ \{ 0, 1\} $.
This generative model is trained using stochastic gradient descent to minimize log loss without a ground truth~\cite{ratner2017snorkel}.
%
%
%
%==============================
\subsection{Experiments}\label{transformers}
%==============================
%
Ontologies are cheaper sources of weak supervision and do not require expert knowledge, while rules are higher cost and generated by experts.
We tested whether adding rule-based labelling sources brought performance gains to ontology-based labelling sources.
We report results on three tiers to gauge the performance changes: 1) UMLS labelling sources, 2) UMLS and non-UMLS labelling sources, 3) UMLS, non-UMLS and exert generated rules. 
It is challenging to build the UMLS-based classifiers for the ``Study type and design'' entity, so we developed only some task-based rules for this entity.


The labelling functions $\lambda_{m}$ were used to label the EBM-PICO training set and obtain $\lambda$. 
We tested majority voting (MV) and label model (LM) to aggregate labelling functions.
LM outputs probabilistic labels for the training set were used as weak supervision signals to train the PubMedBERT with noise-aware cross-entropy loss function.
PubMedBERT was chosen because of its domain similarity to our training data (abstracts from PubMed) and task.
GridSearch was used to fine tune the parameters of the label model using the hand-labelled validation set from the EBM-PICO. % Anjani ( TODO: list the parameters in Appendix)
PubMedBERT was trained on fixed parameters listed in the appendix. % Anjani ( TODO: list the parameters in Appendix)
We did not have a validation set for the study type entity hence we did not optimize on this task.
%
%
%
%==============================
\subsection{Evaluation}\label{eval}
%==============================
%
We report the classical macro-averaged F1 and recall for MV, LM, weakly supervised transformer models and the fully supervised transformer models.
Mean macro-averaged scores are reported over three runs of each of these models with the top three random seeds (0, 1, and 42) used in Python.
The models were separately trained for each target class recognition tasks using the raw (IO) tagging scheme.
We used students t-test with a alpha threshold of 0.05 to measure the statistical significance.
%
%
%
%==============================
\section{RESULTS}\label{results}
%==============================
%
Coarse-grained PICO annotations in EBM-PICO are spans composed of multiple fine-grained subclasses (refer to Table).
We extended these fine-grained classes to design the labelling sources and functions better.
For example, it's easier to search for ontologies representing adverse events or diseases rather than for the entire PIO spans.
Similarly, it is easier to design heuristics separately for outcome terms and instruments of measurement.
For extended subgrouping of the target classes, check the figure~\ref{fig:target_subgroups}.
%For instance, the participant span encompasses participant characteristics: condition (disease, disorder, symptoms), age, gender, ethnicity, language and the sample size in clinical studies.
The intervention span can include intervention name, role (primary intervention or comparator), dosage, frequency, mode of administration and administrator.
The EBM-NLP guidelines restrict annotating to the intervention's name and role (control, placebo).
The outcome span can include the outcome names, the scales, techniques or instruments used to measure them, and the absolute outcome measurement values.
The EBM-NLP guidelines restrict annotating the outcome name and how it was measured.
For a more comprehensive subgrouping, we propose developing a PICO ontology.
%
\begin{figure}[ht]
\centering
\includegraphics[width=0.98\textwidth]{figures/target_subgroups_.pdf}
\caption{\label{fig:target_subgroups} Hierarchical representation of PICO target subclasses}
\end{figure}
%
%
%
\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|l|}
    \hline
        Ontology & target and subcategory \\ 
    \hline
        Disease Ontology DO & P - Condition \\ 
        Human Phenotype Ontology HPO & P - Condition \\ 
        Ontology of adverse events OAE & O - Adverse reactions \\ 
        Chemical entities of biological interest ChEBI & I - Pharma / Drugs \\ 
        Comparative toxicogenomics database - Disease CTD - D & P - Condition \\ 
        Comparative toxicogenomics database - Chemicals CTD - C & I - Pharma / Drugs \\ 
        Gender, Sex, and Sexual Orientation Ontology GSSO & P - Gender, Sex \\ 
        Chemotherapy Toxicities Ontology ONTOTOX & O - Adverse reactions \\ 
        Cancer Care: Treatment Outcomes Ontology CCTOO & O - All \\ 
        Symptoms Ontology SYMP & P - Condition \\ 
        Non-pharmacological interventions ontology NPI & I - Non-pharma / non-drug \\ 
        Nursing Care Coordination Ontology NCCO & I - Non-pharma / non-drug \\ \hline
    \end{tabular}
    \caption{\label{tab:source2targets} List of non-UMLS ontology sources chosen based on their mapping to the respective target subclasses.}
\end{table}


Our task was to design $m$ labelling functions $\lambda^{m}$ corresponding to this fine-grained information, efficiently aggregate them to get probabilistic class labels, and use the weakly-labelled dataset for downstream PICO recognition tasks.
Another aim was to correct the fine-grained PICO annotations errors in the EBM-PICO test set and evaluate the approach on both the EBM-PICO and its updated version.
%==============================
%\subsection{Error Analysis for re-annotation}\label{subsec:err}
%==============================
%
%We used 1\% of the EBM-PICO training set tokens and evaluated for errors individually for PIO classes.
Of 12,960 (1\%) training tokens evaluated to gauge the errors, 4.4\% of the Intervention class tokens, 5.4\% of the Participant class tokens and 4.9\% of the Outcome class tokens were errors.
These errors are categorized for each of the PIO classes and are shown in the Table~\ref{tab:errordist}.
%

\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|r|r|r|}
    \hline
        Error category & Participant & Intervention & Outcome \\ \hline
        Repeat mention unmarked & 213 & 220 & 207 \\ 
        Remain unannotated & 47 & 59 & 71 \\ 
        Inconsistency & 46 & 18 & 85 \\ 
        Punctuation/article & 15 & 23 & 48 \\ 
        Conjunction connector & 30 & 36 & 57 \\ 
        Junk & 53 & 79 & 30 \\ 
        Extra information & 80 & 146 & 58 \\ 
        Generic mention & 90 & 120 & 85 \\ \hline
        Total errors & 574 & 701 & 641 \\ \hline
    \end{tabular}
    \caption{\label{tab:errordist} Error distribution in the analysed tokens (1\%) of EBM-PICO corpus.}
\end{table}

%


Table~\ref{tab:res} reports macro-averaged F1 for weak supervision using ontology labeling functions and those incorporating additional, task-specific rules in comparison to the fully-supervised approach.
Weak supervision performs better on the corrected benchmark by 1.71\% for the participant entity. 


For obvious reasons, the recall always stands higher and the precision lower; therefore, the MV labellers have lower F1.


For all the target classes and tiers, adding task-specific rules boosts F1 between 1.20 to 14.14\%.
Lowest overall improvements were seen in Outcomes target and highest in the intervention target.


Adding non-UMLS ontologies sometimes increased performance and sometimes decreased it across all tiers.


Training PubMedBERT on top of label model increases the F1 performance by x\% - xx\% and has considerable boost for the intervention entity.


Error correction leads to better F1 performance for all the targets, especially for the participant class where both the LM and the weakly supervised F1 overtake the full-supervision score. 


\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
        \hline
        %\Xhline{1pt}
        \multicolumn{3}{|c|}{} &
        \multicolumn{2}{|c|}{MV} & \multicolumn{2}{|c|}{LM} & \multicolumn{2}{|c|}{WS} & \multicolumn{2}{|c|}{FS} \\
        \hline
        Target & LF source & \#LF & Fine & Corr & Fine & Corr & Fine & Corr & Fine & Corr \\
        \hline
        %\Xhline{1pt}
        P & UMLS & 0 & 62.13 & 69.28 & 64.28 & 72.22 & 65.32 & 73.49 & 72.99 & 74.41 \\ 
        ~ & +Ontology & 0 & 61.72 & 69.32 & 64.23 & 72.18 & 64.76 & 72.31 & ~ & ~ \\ 
        ~ & +Rules & 0 & 63.08 & 72.06 & 65.79 & 75.31 & 66.73 & \textbf{76.12} & ~ & ~ \\ \hline
        I/C & UMLS & 0 &59.7 & 63.94 & 60.11 & 64.28 & 59.17 & 61.72 & \textbf{83.37} & 81.06 \\ 
        ~ & +Ontology & 0 & 62.14 & 66.92 & 62.83 & 67.09 & 67.06 & 69.76 & ~ & ~ \\ 
        ~ & +Rules & 0 & 58.51 & 63.45 & 64.34 & 68.17 & 70.27 & 72.39 & ~ & ~ \\  \hline
        O & UMLS & 0 & - & - & 59.53 & 62.47 & running & running & \textbf{81.20} & 80.53 \\ 
        ~ & +Ontology & 0 & - & - & 59.53 & 62.47 & 58.74 & 59.27 & ~ & ~ \\ 
        ~ & +Rules & 0 & - & - & 61.02 & 63.11 & 60.36 & 61.93 & ~ & ~ \\ \hline
    \end{tabular}
    \caption{\label{tab:res} Macro-averaged F1 scores for UMLS, UMLS+other and rule-based weak supervision.}
\end{table}

To investigate the optimal number of UMLS labelling functions required, we used the same methodology as in Trove holding all non-UMLS labeling functions fixed across all ablation tiers and computed performance across partitions = $s = ( 1, 2, \dotso , 127 )$ partitions of the UMLS by terminology obtaining similar results.
The optimal number of partitions mainly lied between two and eight (make a figure for it.
%
%
%
%==============================
\section{DISCUSSION}\label{discussion}
%==============================
%
We demonstrate the effectiveness of weak supervision for a complicated PICO extraction task using medical and non-medical ontologies and expert-generated rules.
It is easy to re-purpose the low-cost ontologies but challenging to map these to our targets.
PICO classes are complex targets encompassing several subclasses, decomposing which allows identifying ontology labelling sources and designing rules. 
Moreover, each tier requires labelling sources that contribute to the target classes.
For example, additional ontologies perform better than tier one only when other ontologies are added for pharmaceutical and non-pharmaceutical interventions.
Similarly, tier two for participant class performs well only if ontologies are added for the participant subclasses like age and gender and not limited to diseases or disorders.
This demonstrates the importance of seen vs unseen entities. 

Label model only takes into account the information encoded into the weak sources to label phrases from the training text but does not take into account the contextual information around this phrase.
The performance boost using PubMedBERT, especially for the Participants class, demonstrates how using downstream contextual models could improve the performance by taking into account the structural properties of text.
they generalize beyond the labelling functions.

The performance boost using task-specific rules shows that PICO tasks could benefit from encoding idiosyncrasies into rules.

%
%
%
%==============================
\subsection{Error Analysis and correction}\label{err_ana}
%==============================
%
We expound on the error categories and provide examples here.
An error falls under \textit{repeated mention} if one instance of an entity is marked, but another identical instance of the same entity in the same context is not marked within the abstract. 
%This error category is the largest.
The reason could be the EBM-PICO guidelines flaw where the annotators of fine-grained entity annotation were confined to only annotate within the longer span-level annotation.
Hence any annotation error missed by the coarse-grained annotators was continued by the fine-grained annotators.

An error falls under \textit{remains unannotated} if a token should have been annotated as an entity but was not.
In the Intervention class, a large portion of this category was constituted by the generic mentions of controls (placebo, saline), which were not annotated.
In the Participant class, patient ethnicity and other information like smoking status and pregnancy information (marked in the coarse-grained span) were not marked in the fine-grained entity.
The reason could be that there was no fine-grained class to categorize this information. 
In the Outcome class, the annotators missed several important outcomes (along with their repeated mention).

\textit{Conjunction connector} errors are the conjunctions occurring between two semantically separate entities but are falsely marked as entities.
For example, ``Nausea and vomiting'' are two separate outcomes marked as one by annotating the conjunction between them.
Sometimes a punctuation succeeding the entity or an article or a preposition preceding the entity was falsely marked as an entity. These fall under \textit{punctuation/article} errors.
Sometimes extraneous tokens were marked along with the entity tokens. 
Such errors fell under \textit{extra information} category.
%For example, in the phrase ``This trial demonstrated short-term efficacy of smokeless tobacco in combination with'', the annotators had marked ``short-term efficacy of smokeless tobacco'' as an outcome entity, but only ``short-term efficacy'' is an outcome entity.
%In contrast ``smokeless tobacco'' is an intervention entity. 
In the intervention class, the annotation guidelines mentioned not annotating any part of the text that did not mention the intervention name.
The annotators often marked extraneous information like intervention dosage, frequency, route of administration and information about the intervention administrator.


A generic reference is a co-reference of an entity mentioned using different or similar (but not identical) words. 
A generic reference of an entity (and its repeated mention) in the same abstract was several times left unmarked by the annotators constituting a \textit{generic reference} error.
For example, if the outcome endpoint ``smoking cessation'' was referred to in the same abstract elsewhere as ``quitting smoking'', it was not marked even though it is a reference to the outcome phrase ``smoking cessation''.
If ``aerobic exercise'' mentioned as ``exercise intervention'' was not marked, this also constitutes a generic reference error.
For instance, in an RCT, if ``breast cancer risk counselling'' intervention was referred to as ``risk counselling'', the former was marked, and the latter was missed.
This error was pronounced specially for the non-pharmaceutical interventions and outcomes.

An \textit{inconsistency} error arises when an entity is fully marked in some abstracts vs in the other abstracts the same entity is partially marked. 
%For example, the intervention description marking the intervention components was marked in some abstracts and unmarked in others.
For example, if an exercise intervention involved aerobic exercise involving stretching and running, this information (``stretching'', ``running'') was marked in some studies while not in the other studies.
In the case of the Participant class, the sample size sub-grouping information was inconsistently marked.
In another example, participant sample size information was either partially or completely marked (``59'' vs ``59 subjects'', ``200'' vs ``200 controls'').
In the Outcome class, the annotation guidelines marked ``what was measured and how it was measured''. 
Several times, the method used for measuring outcomes was inconsistently marked.

A \textit{junk} error is constituted by tokens that were entirely irrelevant for entities but were marked as entities and did not fall into either of the previously mentioned errors.
For example, in the Outcomes evaluation, the phrase ``evaluate and compare'' from the larger phrase ``This study aimed to evaluate and compare'' was marked as an outcome entity even though it is not a valid outcome.


%Why were errors corrected?
These errors and inconsistencies in the EBM-PICO gold test (and training) set can cause faulty evaluation of the machine learning approaches defying the purpose of the corpus.
A possible reason behind these inconsistencies in the EBM-PICO corpus could be that the annotators had clinical background but lacked an informatics background.
This situation could undermine the importance of semantic consistency required for annotating such corpora.
Hiring annotators with the combined knowledge of clinical and informatics domains might improve the manual annotation quality.
The errors for PIO categories were corrected, and the updated dataset is available on Zenodo.
%
%
%
%==============================
\section{CONCLUSION}\label{conclusion}
%==============================
%
OxO
%
%
%
%==============================
\section{Acknowledgements}\label{acknowledgements}
%==============================
%
OxO
%
%
%
%==============================
\section{QUESTIONS TO ADDRESS}\label{ques}
%==============================
%
\begin{enumerate}
    \item What annotation effort or how much annotation effort will be saved using distant-pico?
    \item Problem of high precision and low recall. Could it solved by reducing the abstains?
\end{enumerate}
%
%
%
%==============================
\bibliographystyle{vancouver}
\bibliography{literature}
%==============================

\end{document}