\documentclass[10.7pt,]{article}

\usepackage[letterpaper, margin=2.54cm, top=2.54cm]{geometry}
\usepackage[super,comma,sort&compress]{natbib}
\usepackage{lmodern}
\usepackage{authblk} % To add affiliations to authors
\usepackage{amssymb,amsmath}
\usepackage{wrapfig}
\usepackage{graphicx,grffile}
\usepackage[labelfont=bf,labelsep=period]{caption}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
    \setmainfont[]{Arial Narrow}
    \setsansfont[]{Century Gothic}
    \setmonofont[Mapping=tex-ansi]{Consolas}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
	\usepackage{microtype}
	\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}

\usepackage{lipsum} % for dummy text only REMOVE

\newtheorem{exm}{Example}


%==============================
% Customization to make the output PDF 
% look similar to the MS Word version
%==============================
% To prevent hyphenation
\hyphenpenalty=10000
\exhyphenpenalty=10000

% To set the sections font size
\usepackage{sectsty}
\allsectionsfont{\fontsize{11}{11}\selectfont}
\sectionfont{\fontsize{14}{14}\selectfont}
\subsectionfont{\bfseries\fontsize{13}{13}\selectfont}
\subsubsectionfont{\bfseries\fontsize{11}{11}\selectfont}
%\subsubsectionfont{\normalfont}

% Spacing
\usepackage{setspace}


% No new line after subsubsection
\makeatletter
%\renewcommand\subsubsection{\@startsection{subsubsection}{3}{\z@}%
%	{-3.25ex\@plus -1ex \@minus -.2ex}%
%    {-1.5ex \@plus -.2ex}% Formerly 1.5ex \@plus .2ex
%    {\normalfont}}
%\makeatother

\makeatletter % Reference list option change
\renewcommand\@biblabel[1]{#1.} % from [1] to 1
\makeatother %

% To set the doc title font
\usepackage{etoolbox}
\makeatletter
\patchcmd{\@maketitle}{\LARGE}{\bfseries\fontsize{15}{16}\selectfont}{}{}
\makeatother

% No page numbering
\pagenumbering{gobble}

\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother

% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
%==============================
\usepackage{hyperref}
\hypersetup{
	unicode=true,
	pdftitle={My Cool Title Here},
	pdfauthor={Author One, Author Two, Author Three},
	pdfkeywords={keyword1, keyword2},
	pdfborder={0 0 0},
	breaklinks=true
}
\urlstyle{same}  % don't use monospace font for urls

% Keywords command
\providecommand{\keywords}[1]
{
  \small	
  \textbf{Key words---} #1
}
%==============================

% reduce space between title and begining of page
\title{\vspace{-2em} Not So Weak-PICO: Leveraging weak supervision for Participants, Interventions, and Outcomes extraction for systematic review automation}
\date{\vspace{-5ex}}
\author[ ] {
    % Authors
    \bf\fontsize{13}{14}\selectfont
    Anjani Dhrangadhariya,\textsuperscript{\rm 1, 2}
    Henning M\"uller \textsuperscript{\rm 1, 2}
}
\affil[1]{Institute of Business Information Systems, University of Applied Sciences Western Switzerland (HES-SO Valais-Wallis), Sierre, Switzerland}
\affil[2]{Department of Computer Science, University of Geneva (UNIGE), Geneva, Switzerland}
\affil[*]{Corresponding author: Anjani Dhrangadhariya, Institute of Business Information Systems, University of Applied Sciences Western Switzerland (HES-SO Valais-Wallis), Sierre, Switzerland; anjani.dhrangadhariya@hevs.ch}
%==============================
\begin{document}
\maketitle
\vspace{2em} %separation between the affiliations and abstract
%==============================
\doublespacing
%==============================
\section{ABSTRACT}
\label{abstract}
%==============================
% Note: Abstract is limited to 250 words
%
\textbf{Objective:}
PICO entity extraction is a vital but time-consuming task for conducting systematic reviews. 
Supervised machine learning methods can help fully automate PICO entity extraction, but a lack of large annotated corpora restricts innovation and adoption of automated PICO recognition systems.
The largest-available PICO entity corpus is manually annotated, which is too expensive for the most scientific community.
Moreover, even the well-trained annotators disagree on the exact spans or entities constituting PICO mentions, often requiring resource-intensive post-annotation corrections.
Our objective is to investigate a weak supervision approach devising ontologies and expert-generated rules for PICO entity extraction without relying on hand-labelled data.\\
\textbf{Materials and Methods:}
Ontologies - UMLS + more, dictionaries and expert-generated rules for designed for PICO recognition along with weakly supervised models.\\
\textbf{Results:}
We present Weak-PICO+, a method for weakly supervised PICO entity recognition using medical and non-medical ontologies and expert-generated rules.
Unlike manual annotation, Weak-PICO+ does not use any hand-labelled data and is quickly adaptable and extensible to other entities.
We demonstrate this through extracting an additional Study Type entity making PICO go PICOS without manual annotation effort.\\
\textbf{Conclusion:}
Weak supervision using weak-PICO+ for PICO entity recognition is not only feasible but also outperforms full supervision.\\
%
%
%


\keywords{Randomized Controlled Trials, Weakly-Supervised Machine Learning, Information Extraction, Evidence-based Medicine}
%
\clearpage
%
%
%
%==============================
\section{INTRODUCTION}\label{introduction}
%==============================
%
Systematic Review (SR) is an evidence-based practice of answering clinical questions using a transparent and quantitative approach whereby the reviewers must collect as many research publications as possible, identifying the relevant ones from these and integrating their results via statistical analysis.
Filtering relevant publications from the bulk collected ones often uses PICO (Participants, Interventions, Comparators, Outcomes) criteria. 
A clinical trial or a publication is relevant only if studies question relevant PICO information. 
Manually analysing PICO information from many publications for a single SR takes about 12 months of two medical experts' time.
The process can be automated using machine learning based information extraction strategies by directly pointing the human reviewers to the correct PICO descriptions.
Information extraction strategies like machine learning classifiers rely on hand-labelled training corpora.
Manually labelling corpora with PICO information for a downstream IE application requires people with combined medical and informatics skills, is expensive and is time-consuming in terms of the annotators' actual annotation process and training.
Moreover, depending upon the systematic review question, PICO criteria are extended to PICOS (S-Study design), PICOC (C-Context), and PICOT (T-timeframe).
Hand-labelled datasets are static and expensive to adapt to a fast-changing real-world scenario. 


Annotation bottleneck has shifted gears in favour of weakly-supervised learning that relies on programmatic labelling sources to obtain training data.
Not only is programmatic labelling quick and is efficiently modified as per the downstream application changes.
Medical ontology compendium like UMLS is an excellent resource for building 






\begin{itemize}
    \item PICO recognition - define, importance in SRs, inclusion/exclusion criteria
    \item Parts of PICO form a part of inclusion/exclusion criteria
    \item Extension - PICOS, PICOTS, PICO+, Extension of the part of PICOS
    \item Corpus annotation challenges, slow process
    \item Lack of large corpora, not easy extension = Bottleneck to innovation
    \item Data-centric AI, weak supervision, distant supervision = solution
    \item Current approaches to PICO = limited to spans not entities
    \item entities = semantic units vs. span = maximum information/description
    \item This limits full supervision because once the spans are identified, they still require digging deeper to identify semantic entity corresponding to semantic entities
    \item Approaches going beyond span recognition to address entity recognition are required
    \item Entity recognition approach to PICO is not as easy as entity recognition approach to disease or chemical names which are more or less standardised. PICO terms are not standard and even the experts do not agree on the exact span constituting PICO.
    \item We present a data-centric, weakly-supervised approach to PICO entity recognition using medical ontologies, hand-crafted dictionaries, distant supervision and expert-generated rules. 
    %\item We extend it to an additional entity to show the utility of weakly-supervision entity recognition for PICOS (S = Study type) entities.
\end{itemize}
%
%
%
%==============================
\section{METHODOLOGY}\label{methods}
%==============================
%
The birds-eye view of our approach is shown in the Figure.
%
%
%
%==============================
\subsection{Datasets and tasks}\label{data}
%==============================
%
\textbf{EBM-PICO.}
The EBM-PICO dataset developed by Nye \textit{et al.} consists of about 5000 PICO entity/span annotated documents.~\footnote{A single document consists of a title and an abstract.}
It comes pre-divided into a training set (n=4,933) annotated through crowd-sourcing and an expert annotated gold test set (n=191) for evaluation purposes.
The dataset has annotations at two levels: span-level coarse-grained PICO and entity-level fine-grained PICO.
Span level PICO annotation encompasses the maximum amount of PICO descriptions that often span an entire sentence.
Entity-level PICO annotation covers fine-grained PICO information at the entity level, with PICO classes further divided into fine-grained subclasses.



\textbf{EBM-PICO corrected}
The EBM-PICO annotation guidelines caution about variable annotation quality~\footnote{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6174533/bin/NIHMS988059-supplement-Appendix.pdf}.
Variable annotation quality in the training dataset is pardonable, but the possible errors in the gold standard test set will impact the development of downstream PICO extraction models.
The entity-level PICO annotations in the EBM-PICO gold test set (n=191) have several errors that could lead to faulty evaluation of machine learning methods.
We evaluate 1\% of the EBM-PICO training set tokens to gauge the possible reasons for the errors.
We use this error evaluation exercise to conduct an error-focused corpus re-annotation for the EBM-PICO gold corpus (n = 191) for PICO entities.
%These errors emanate from the annotation process of EBM-PICO corpus that follows the sequence of first annotating only the span-level PICO information and then restrict the entity-level PICO annotation to these larger spans.
%This leads to specifically missing out on the the repeated mentions of the fine-grained PICO information that is not covered in the longer spans. 
%We annotated an additional entity for study type only for this corpus to measure the effectiveness of weak supervision approaches.
%Our study type entity is restricted to whether a study is a ``Randomized Controlled Trial'' (RCT) or not.
%An RCT can be classified based on trial design as parallel group, cross-over, cluster or factorial trial, on outcome of interest as pragmatic or exploratory trial and on hypothesis as superiority vs. noninferiority vs. equivalence trials.
%This annotation exercise was to demonstrate the application of weak supervision for entities beyond PICO~\cite{menard2019turning}.

\textbf{Physio set.} A test set comprising 153 PICO entity/span annotated documents from Physiotherapy and Rehabilitation RCTs (Randomized Controlled Trials) was used as an additional benchmark to evaluate the generalization power of our approach for this sub-domain~\cite{dhrangadhariya2021end}.

None of these datasets underwent any preprocessing except tagging them with the POS tags using spaCy.

Our ontology labelling sources are UMLS, comparative toxicogenomics database (CTD) disease, disease ontology DO, Human Phenotype Ontology (HPO), Ontology of adverse events (OAE), Chemical entities of biological interest (ChEBI),  comparative toxicogenomics database (CTD) Chemical and Disease subclasses.
We used the 2021AB-full release of UMLS Metathesaurus English subset dump removing non-English and zoonotic source terminologies as well as sources containing fewer than 500 terms remaining with 131 vocabularies.
Our distant supervision dictionaries were created from the structure fields of clinicaltrials.gov .
Our hand-coded dictionaries were for gender, disease abbreviation dictionary, intervention comparator terms, and outcome endpoints from the sources like XX, XX, XX, respectively.

Preprocessing of UMLS ontologies: Remove stopwords (NLTK, Gensim, scikit learn, spaCy), numbers and punctuation's.



\begin{table}[h!]
\begin{center}
\begin{tabular}{| c | c | c | c |} 
\hline
 & P & I/C & O \\ 
\hline
0 & No label & No label & No label \\ 
1 & Age & Surgical & Physical \\ 
2 & Sex & Physical & Pain \\
3 & Sample size & Drug & Mortality \\
4 & Condition & Educational & Side effect \\
5 &  & Psychological & Mental \\
6 &  & Other & Other \\
7 &  & Control &  \\
\hline
\end{tabular}
\caption{P (Participant), I (Intervention) and O (Outcome) represent the coarse-grained labels which are further divided into respective fine-grained labels.}
\label{table:coarsefineconcept}
\end{center}
\end{table}
%
%
%
%==============================
\subsection{Sequence labeling}\label{data}
%==============================
%
PICO entity corpus creation is a token labeling problem where each token in a sequence X is classified or labeled into one of the three PICO classes (Y).
Y is not observable so our challenge was to devise data-centric techniques to get token labels corresponding to PICO classes and sub-classes and create a labeled dataset.
Since Y is not observable, our primary technical challenge is estimating Y from multiple, potentially conflicting label sources of unknown quality to construct a probabilistically labeled dataset.
This dataset can then be used for training classification models such as deep neural networks.
Such a labeling regimen is typically low-cost, but less accurate than the hand-curated labels used in traditional supervised learning; hence, this paradigm is referred to as weakly supervised learning.
%
%
%
%==============================
\subsection{Labeling functions}\label{lfs}
%==============================
%
UMLS concepts as labeling functions. It is difficult to classify different Semantic groups as positive signals for the PICOS classes. For example, the Participant span can include information about different participant characteristics: disease, disorder, ethnicity, language or geographical location, age, gender, sex,... 
%
%
%
%==============================
\subsection{Label Model}\label{lfs}
%==============================
%
Combine multiple labeling function results into probablistic model.
%
%
%
%==============================
\subsection{Transformer}\label{lfs}
%==============================
%
Training end-models for weakly supervised and fully supervised learning: PubMedBERT, SciBERT, BioBERT
%
%
%
%==============================
\subsection{EVALUATION}\label{eval}
%==============================
%
We report the classical macro-averaged F1-scores for the label models, weakly supervised transformer models and the fully supervised transformer models.
Mean macro-averaged F1-scores are reported over three runs of each of these models with the top three random seeds (0, 1, and 42) used in Python.
These models were separately trained for each of the PIO entity recognition tasks using the raw (IO) tagging scheme.
We used students t-test with a alpha threshold of 0.05 to measure the statistical significance.
%
%
%
%==============================
\section{RESULTS}\label{results}
%==============================
%
Table demonstrating the scores mentioned above.
%
%
%
%==============================
\subsection{Error Analysis for re-annotation}\label{subsec:err}
%==============================
%
We used 1\% of the EBM-PICO training set tokens and evaluated for errors individually for PIO classes.
of 12, 960 tokens evaluated, 4.4\% of the Intervention class tokens, 5.4\% of the Participant class tokens and 4.9\% of the Outcome class tokens were errors.
These errors are categorized for each of the PIO classes and are shown in the Table~\ref{tab:errordist}.
%

\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|r|r|r|}
    \hline
        Error category & Participant & Intervention & Outcome \\ \hline
        Repeat mention unmarked & 213 & 220 & 207 \\ 
        Remain unannotated & 47 & 59 & 71 \\ 
        Inconsistency & 46 & 18 & 85 \\ 
        Punctuation/article & 15 & 23 & 48 \\ 
        Conjunction connector & 30 & 36 & 57 \\ 
        Junk & 53 & 79 & 30 \\ 
        Extra information & 80 & 146 & 58 \\ 
        Generic mention & 90 & 120 & 85 \\ \hline
        Total errors & 574 & 701 & 641 \\ \hline
    \end{tabular}
    \caption{tab:errordist}
    \label{Error distribution in the analysed tokens of EBM-PICO corpus.}
\end{table}

%
\iffalse
\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|r|l|r|l|r|}
    \hline
    \multicolumn{2}{|c|}{Intervention} & \multicolumn{2}{|c|}{Participant} & \multicolumn{2}{|c|}{Outcome} \\
    \hline
    Error type & Count & Error type & Count & Error type & Count \\
    \hline
        Description inconsistency & 18 & Remain unannotated & 93 & Scale inconsistency & 85 \\
        Period/article & 23 & Period/article & 15 & Period/article & 48 \\ 
        Unmarked control & 59 & Extra info marked & 80 & Extra info marked & 58 \\ 
        Junk information & 225 & Junk information & 53 & Junk information & 30 \\ 
        Generic name & 120 & Generic name & 90 & Generic name & 85 \\ 
        Conjunction connector & 36 & Conjunction connector & 30 & Conjunction connector & 57 \\ 
        Repeated mention & 220 & Repeated mention & 213 & Repeated mention & 207 \\
        - & - & - & - & Outcomes not marked & 71 \\ 
        Total errors & 701 & Total errors & 574 & Total errors & 641 \\ 
        %Total tokens evaluated & 12960 & Total tokens evaluated & 12960 & Total tokens evaluated & 12960 \\
        \hline
        \multicolumn{3}{|l|}{Total tokens evaluated} & \multicolumn{3}{|c|}{12960}\\
        \hline
    \end{tabular}
    \caption{\label{tab:errordist} Error distribution in the analysed tokens of EBM-PICO corpus.}
\end{table}
\fi

%
%
%
%==============================
\section{DISCUSSION}\label{discussion}
%==============================
%
%==============================
\subsection{Error Analysis and correction}\label{err_ana}
%==============================
%
We expound on the error categories and provide examples here.
%The error categories Repeated mention, conjunction connector, generic reference, extra info marked and period/article are the shared error categories across the classes, while the rest are class specific.
An error falls under \textit{repeated mention} if one instance of an entity is marked, but another identical instance of the same entity is not marked within the same abstract. 
This error category is the largest.

An error falls under \textit{remains unannotated} if a token should have been annotated as an entity but was not.
In the Intervention class, a large portion of this category was constituted by the generic mentions of controls (placebo, saline), which were not annotated.
In the Participant class, patient ethnicity and other information like smoking status and pregnancy information (marked in the coarse-grained span) were not marked in the fine-grained entity.
The reason could be that there was no fine-grained class to categorize this information. 
In the Outcome class, the annotators missed several important outcomes (along with their repeated mention).

\textit{Conjunction connector} errors are the conjunctions occurring between two semantically separate entities but are falsely marked as entities.
For example, ``Nausea and vomiting'' are two separate outcome entities marked as one by annotating the conjunction between them.

Sometimes extra punctuation succeeding the entity or an article or a preposition preceding the entity was falsely marked as an entity. These fall under \textit{punctuation/article} errors.

Sometimes extraneous tokens were marked along with the entity tokens. 
Such errors fell under \textit{extra information} error category.
For example, in the phrase ``This trial demonstrated short-term efficacy of smokeless tobacco in combination with'', the annotators had marked ``short-term efficacy of smokeless tobacco'' as an outcome entity, but only ``short-term efficacy'' is an outcome entity. In contrast ``smokeless tobacco'' is an intervention entity. 
In the intervention class, the annotation guidelines mentioned not to annotate any part of the text that did not mention the intervention name, but a lot of the times, the annotators marked extraneous information like intervention dosage, frequency, route of administration and information about the intervention administrator.


A generic reference is a co-reference of an entity mentioned using different or similar (but not identical) words. 
A generic reference of an entity (and its repeated mention) in the exact abstract was several times left unmarked by the annotators constituting a \textit{generic reference} error.
For example, if the outcome endpoint ``smoking cessation'' was referred to in the same abstract elsewhere as ``quitting smoking'', it was not marked even though it is a reference to the outcome phrase ``smoking cessation''.
If ``aerobic exercise'' mentioned as ``exercise intervention'' was not marked, this also constitutes a generic reference error.
For instance, in an RCT, if ``breast cancer risk counselling'' intervention was referred to as ``risk counselling'', the former was marked, and the latter was missed.
This error was pronounced specially for the non-pharmaceutical interventions and outcomes.

An \textit{inconsistency} error arises when an entity is fully marked in some abstracts vs in the other abstracts the same entity is partially marked. 
For example, the intervention description marking the intervention components was marked in some abstracts and unmarked in others.
For example, if the exercise intervention involved aerobic exercise involving stretching and running, this information (``stretching'', ``running'') was marked in some studies while not in the other studies.
In the case of the Participant class, the sample size sub-grouping information was sometimes marked and sometimes left unmarked.
In another example, participant sample size information was either partially marked or completely marked (``59'' vs ``59 subjects'', ``200'' vs ``200 controls'').
In the Outcome class, the annotation guidelines marked ``what was measured and how it was measured''. 
Several times, the method used for measuring outcomes was inconsistently marked.

A \textit{junk} error is constituted by tokens that were entirely irrelevant for entities but were marked as entities and did not fall into either of the previously mentioned errors.
For example, in the Outcomes evaluation, the phrase ``evaluate and compare'' from the larger phrase ``This study aimed to evaluate and compare'' was marked as an outcome entity even though it is not a valid outcome endpoint.


%Why were errors corrected?
These errors and inconsistencies in the EBM-PICO gold test (and training) set can cause faulty evaluation of the machine learning approaches defying the purpose of the corpus.
The reason behind these inconsistencies in the EBM-PICO corpus could be that the annotators had clinical background but lacked an informatics background.
This situation could undermine the importance of semantic consistency required for annotating such corpora.
Hiring annotators with the combined knowledge of clinical and informatics domains might improve the manual annotation quality.
The errors for PIO categories were corrected, and the updated dataset is available on Zenodo.
Another reason could be the EBM-PICO guidelines flaw where the annotators of fine-grained entity annotation were confined to only annotate within the longer span-level annotation.
Hence any annotation error missed by the coarse-grained annotators was continued by the fine-grained annotators.
%
%
%
%==============================
\section{CONCLUSION}\label{conclusion}
%==============================
%
OxO
%
%
%
%==============================
\section{Acknowledgements}\label{acknowledgements}
%==============================
%
OxO
%
%
%
%==============================
\section{QUESTIONS TO ADDRESS}\label{ques}
%==============================
%

\begin{enumerate}
    \item Is it important to have high F1 score for the individual labeling functions?
    \item Similar question: Will individual LF false negatives impact the performance?
    \item What would you call low coverage? What should be the ideal coverage of the labeling functions?
    \item What if higher coverage leads to higher False Positives?
    \item Labeling function weights for the functions with high F1 score (and not a good coverage necessarily)?
    \item How is overlap calculated? are only positive classes calculated in the overlap? > Coverage is calculated over 0 and 1 labels excluding the -1 labels.
    \item Is coverage correlated to F1 score?
    \item How is abstain treated while calculating F1 score?
    \item Check out Trove for how "0" labels are calculated?
    \item What annotation effort or how much annotation effort will be saved using distant-pico?
\end{enumerate}

Learning rate for label model train should be lower. LR like 1.0, 0.5, 0.1, 0.05, 0.01 did not work for me.

Problem of high precision and low recall. Could it solved by reducing the abstains?
%
%
%
%==============================
\bibliographystyle{vancouver}
\bibliography{literature}
%==============================

\end{document}