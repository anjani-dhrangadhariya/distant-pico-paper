\documentclass[10.7pt,]{article}

\usepackage[letterpaper, margin=2.54cm, top=2.54cm]{geometry}
\usepackage[super,comma,sort&compress]{natbib}
\usepackage{lmodern}
\usepackage{authblk} % To add affiliations to authors
\usepackage{amssymb,amsmath}
\usepackage{wrapfig}
\usepackage{graphicx,grffile}
\usepackage[labelfont=bf,labelsep=period]{caption}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
    \setmainfont[]{Arial Narrow}
    \setsansfont[]{Century Gothic}
    \setmonofont[Mapping=tex-ansi]{Consolas}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
	\usepackage{microtype}
	\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}

\usepackage{lipsum} % for dummy text only REMOVE

\newtheorem{exm}{Example}


%==============================
% Customization to make the output PDF 
% look similar to the MS Word version
%==============================
% To prevent hyphenation
\hyphenpenalty=10000
\exhyphenpenalty=10000

% To set the sections font size
\usepackage{sectsty}
\allsectionsfont{\fontsize{11}{11}\selectfont}
\sectionfont{\fontsize{14}{14}\selectfont}
\subsectionfont{\bfseries\fontsize{13}{13}\selectfont}
\subsubsectionfont{\bfseries\fontsize{11}{11}\selectfont}
%\subsubsectionfont{\normalfont}

% Spacing
\usepackage{setspace}


% No new line after subsubsection
\makeatletter
%\renewcommand\subsubsection{\@startsection{subsubsection}{3}{\z@}%
%	{-3.25ex\@plus -1ex \@minus -.2ex}%
%    {-1.5ex \@plus -.2ex}% Formerly 1.5ex \@plus .2ex
%    {\normalfont}}
%\makeatother

\makeatletter % Reference list option change
\renewcommand\@biblabel[1]{#1.} % from [1] to 1
\makeatother %

% To set the doc title font
\usepackage{etoolbox}
\makeatletter
\patchcmd{\@maketitle}{\LARGE}{\bfseries\fontsize{15}{16}\selectfont}{}{}
\makeatother

% No page numbering
\pagenumbering{gobble}

\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother

% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
%==============================
\usepackage{hyperref}
\hypersetup{
	unicode=true,
	pdftitle={My Cool Title Here},
	pdfauthor={Author One, Author Two, Author Three},
	pdfkeywords={keyword1, keyword2},
	pdfborder={0 0 0},
	breaklinks=true
}
\urlstyle{same}  % don't use monospace font for urls

% Keywords command
\providecommand{\keywords}[1]
{
  \small	
  \textbf{Key words---} #1
}
%==============================

% reduce space between title and begining of page
\title{\vspace{-2em} Not So Weak-PICO$+$: Leveraging weak supervision for Patient, Interventions, Outcomes and Study type extraction for systematic review automation}
\date{\vspace{-5ex}}
\author[ ] {
    % Authors
    \bf\fontsize{13}{14}\selectfont
    Anjani Dhrangadhariya,\textsuperscript{\rm 1, 2}
    Henning M\"uller \textsuperscript{\rm 1, 2}
}
\affil[1]{Institute of Business Information Systems, University of Applied Sciences Western Switzerland (HES-SO Valais-Wallis), Sierre, Switzerland}
\affil[2]{Department of Computer Science, University of Geneva (UNIGE), Geneva, Switzerland}
\affil[*]{Corresponding author: Anjani Dhrangadhariya, Institute of Business Information Systems, University of Applied Sciences Western Switzerland (HES-SO Valais-Wallis), Sierre, Switzerland; anjani.dhrangadhariya@hevs.ch}
%==============================
\begin{document}
\maketitle
\vspace{2em} %separation between the affiliations and abstract
%==============================
\doublespacing
%==============================
\section{ABSTRACT}
\label{abstract}
%==============================
% Note: Abstract is limited to 250 words
%
\textbf{Objective:}
PICO entity extraction is a vital but time-consuming task for conducting systematic reviews. 
Supervised machine learning methods can help fully automate PICO entity extraction, but a lack of large annotated corpora restricts innovation and adoption of automated PICO recognition systems.
The largest-available PICO entity corpus is manually annotated, which is too expensive for the most scientific community.
Moreover, even the well-trained annotators disagree on the exact spans or entities constituting PICO mentions, often requiring resource-intensive post-annotation corrections.
Our objective is to investigate a weak supervision approach devising ontologies and expert-generated rules for PICO entity extraction without relying on hand-labelled data.\\
\textbf{Materials and Methods:}
Ontologies - UMLS + more, dictionaries and expert-generated rules for designed for PICO recognition along with weakly supervised models.\\
\textbf{Results:}
We present Weak-PICO+, a method for weakly supervised PICO entity recognition using medical and non-medical ontologies and expert-generated rules.
Unlike manual annotation, Weak-PICO+ does not use any hand-labelled data and is quickly adaptable and extensible to other entities.
We demonstrate this through extracting an additional Study Type entity making PICO go PICOS without manual annotation effort.\\
\textbf{Conclusion:}
Weak supervision using weak-PICO+ for PICO entity recognition is not only feasible but also outperforms full supervision.\\
%
%
%


\keywords{Randomized Controlled Trials, Weakly-Supervised Machine Learning, Information Extraction, Evidence-based Medicine}
%
\clearpage
%
%
%
%==============================
\section{INTRODUCTION}\label{introduction}
%==============================
%
\begin{itemize}
    \item PICO recognition - define, importance in SRs, inclusion/exclusion criteria
    \item Parts of PICO form a part of inclusion/exclusion criteria
    \item Extension - PICOS, PICOTS, PICO+, Extension of the part of PICOS
    \item Corpus annotation challenges, slow process
    \item Lack of large corpora, not easy extension = Bottleneck to innovation
    \item Data-centric AI, weak supervision, distant supervision = solution
    \item Current approaches to PICO = limited to spans not entities
    \item entities = semantic units vs. span = maximum information/description
    \item This limits full supervision because once the spans are identified, they still require digging deeper to identify semantic entity corresponding to semantic entities
    \item Approaches going beyond span recognition to address entity recognition are required
    \item Entity recognition approach to PICO is not as easy as entity recognition approach to disease or chemical names which are more or less standardised. PICO terms are not standard and even the experts do not agree on the exact span constituting PICO.
    \item We present a data-centric, weakly-supervised approach to PICO entity recognition using medical ontologies, hand-crafted dictionaries and expert-generated rules. 
    \item We extend it to an additional entity to show the utility of weakly-supervision entity recognition for PICOS (S = Study type) entities.
\end{itemize}
%
%
%
%==============================
\section{MATERIALS AND METHODS}\label{methods}
%==============================
%
\begin{itemize}
    \item datasets - EBM-PICO training, EBM-PICO test gold, Physio gold
    \item Labeling function design - Positive LF, Negative LF
    \item Candidate generation - Experiments
    \item Model training - Training models using strong, weak and combination annotations. - Experiments
    \item Evaluation
\end{itemize}
%
%
%
%==============================
\subsection{Datasets}\label{data}
%==============================
%
\textbf{EBM-PICO.}
The EBM-PICO dataset developed by Nye \textit{et al.} consists of 5000 PICO entity/span annotated documents.~\footnote{A single document consists of a title and an abstract.}
It comes pre-divided into a training set (n=4,933) annotated through crowd-sourcing and an expert annotated gold test set (n=191) for evaluation purposes.
The dataset has annotations at two levels.
Span level PICO annotation encompasses the maximum amount of PICO descriptions that often span an entire sentence.
Entity-level PICO annotation cover fine-grained PICO information at the entity level with PICO classes further divided into fine-grained subclasses.



\textbf{EBM-PICO corrected + Study Type.}
The EBM-PICO annotation guidelines warn about variable annotation quality of the annotations~\footnote{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6174533/bin/NIHMS988059-supplement-Appendix.pdf}.
Variable annotation quality in the training dataset is pardonable, but the possible errors in the gold standard test sets will impact the development of downstream PICO extraction models.
The fine-grained PICO annotations in EBM-PICO gold test set (n=191), however, have several errors that lead to faulty evaluation of machine learning methods.
These errors emanate from the annotation process of EBM-PICO corpus that follows the sequence of first annotating only the span-level PICO information and then restrict the entity-level PICO annotation to these larger spans.
This leads to specifically missing out on the the repeated mentions of the fine-grained PICO information that is not covered in the longer spans. 
We conducted an error-focused corpus re-annotation for the EBM-PICO gold corpus (n = 191) for PICO entities.
We annotated an additional entity for study type only for this corpus to measure the effectiveness of weak supervision approaches.
Our study type entity is restricted to whether a study is a ``Randomized Controlled Trial'' or not.
This annotation exercise was to demonstrate the application of weak supervision for entities beyond PICO.

\textbf{Physio set.} A test set comprising 153 PICO entity/span annotated documents from Physiotherapy and Rehabilitation RCTs (Randomized Controlled Trials) was used as an additional benchmark to evaluate the generalization power of our approach for this sub-domain


\begin{table}[h!]
\begin{center}
\begin{tabular}{| c | c | c | c |} 
\hline
 & P & I/C & O \\ 
\hline
0 & No label & No label & No label \\ 
1 & Age & Surgical & Physical \\ 
2 & Sex & Physical & Pain \\
3 & Sample size & Drug & Mortality \\
4 & Condition & Educational & Side effect \\
5 &  & Psychological & Mental \\
6 &  & Other & Other \\
7 &  & Control &  \\
\hline
\end{tabular}
\caption{P (Participant), I (Intervention) and O (Outcome) represent the coarse-grained labels which are further divided into respective fine-grained labels.}
\label{table:coarsefineconcept}
\end{center}
\end{table}


%
%
%
%==============================
\subsection{Labeling functions}\label{lfs}
%==============================
%
UMLS concepts as labeling functions. It is difficult to classify different Semantic groups as positive signals for the PICOS classes. For example, the Participant span can include information about different participant characteristics: disease, disorder, ethnicity, language or geographical location, age, gender, sex,... 
%
%
%
%==============================
\subsection{EVALUATION}\label{eval}
%==============================
%
\begin{itemize}
    \item Classic metrics (Precision, Recall, F1).
    \item No evaluation for the Study type entity
    \item Students t-test for significance
\end{itemize}
%
%
%
%==============================
\section{RESULTS}\label{results}
%==============================
%
Table demonstrating the scores mentioned above.
%
%
%
%==============================
\subsection{Error Analysis}\label{subsec:err}
%==============================
%
%The EBM-PICO gold test set was evaluated for errors individually for PICO entities and the results are shown in Table.
The EBM-PICO training set was evaluated for errors individually for PICO classes.
A total of 5.4\% of all the 12,960 tokens were errors in the part of the EBM-PICO training corpus for the ``Intervention'' class.
A total of 4.2\% of all the 12,960 tokens were errors in the part of the EBM-PICO training corpus for the ``Participant'' class.
The error categories are shown in the Table~\ref{tab:errordist}.


\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|r|l|r|}
    \hline
    \multicolumn{2}{|c|}{Intervention} & \multicolumn{2}{|c|}{Participant} \\
    \hline
    Error type & Count & Error type & Count\\
    \hline
        Description inconsistency & 18 & Should have been annotated & 93 \\
        Period/article & 23 & Period/article & 15 \\ 
        Unmarked control & 59 & - & - \\ 
        Junk information & 225 & Junk information & 113 \\ 
        Generic name & 120 & Generic name & 90 \\ 
        Conjunction connector & 36 & Conjunction connector & 30 \\ 
        Repeated mention & 220 & Repeated mention & 213 \\ 
        Total & 701 & Total & 554 \\ 
        Total tokens evaluated & 12960 & Total tokens evaluated & 12960 \\ \hline
    \end{tabular}
    \caption{\label{tab:errordist} Your caption.}
\end{table}
%
%
%
%==============================
\section{DISCUSSION}\label{discussion}
%==============================
%
\paragraph{Error Analysis and correction: } 

Errors and the error correction to the EBM-PICO gold set explained here. 

A major chunk fell under ``junk information'' category.
In the intervention class, the annotation guidelines clearly mentioned not to annotate any part of text that did not mention intervention name but a lot of the times, extraneous information like intervention dosage, frequency, route of administration and the information about intervention administrator were marked. 
The most common error across all the entities was the annotators missing out on repeated mention of an entity especially for the non-pharmaceutical interventions. 
This again stemmed from the annotation guidelines where the ``entity level'' annotation was supposed to be done only within the longer span level annotation.
Many span level annotations did not capture the repeated, co-referred or generic mentions of the entities.
For instance, for a RCT with \textit{breast cancer risk counselling} as an intervention referred to it as \textit{risk counselling}, the former was marked but the later was missed.
Sometimes intervention description marking the components of intervention was marked and sometimes it was not marked.
For example, if the exercise intervention involved aerobic exercise along with stretching, this information was marked in some studies while not in the other studies.
The description of control group entities was inconsistently marked as well.

for the ``Participants'' class, the errors stemmed from two main categories.
Repeated mention was not marked including the repeated mention of an disease abbreviation.
Generic mention of the entity was not marked for example Primary breast cancer mention as just breast cancer was not marked. 
Demographic information was not annotated.
Other information about the patients like smoker, pregnant was not annotated.
Patient subgroup information was not annotated as well.
Bracket around the abbreviations were marked.
A lot of junk information was marked as well. Mention examples of junk information here.
Some generic information was inconsistently marked. For example, for the sample size sub-entity, sometimes ``59 healthy patients'' was marked whilst sometimes only ``59'' was marked.




Some of the common errors stemmed by annotating either the punctuation following the entity or the articles/prepositions before the entity even though they were not the part of that entity.
Even the conjunctions between subsequent entities were marked even though they occurred between two separate semantic entities.


Why were errors corrected?
Errors like this can cause faulty evaluation of the machine learning issues when one mention is annotated but another is not annotated.
We suggest the annotators to be medical students with knowledge about informatics like Bioinformaticians given that the ultimate application of such corpora is to develop machine learning algorithms that are in accordance to .
%
%
%
%==============================
\section{CONCLUSION}\label{conclusion}
%==============================
%
OxO
%
%
%
%==============================
\section{Acknowledgements}\label{acknowledgements}
%==============================
%
OxO
%
%
%
%==============================
\section{QUESTIONS TO ADDRESS}\label{ques}
%==============================
%

\begin{enumerate}
    \item Is it important to have high F1 score for the individual labeling functions?
    \item Similar question: Will individual LF false negatives impact the performance?
    \item What would you call low coverage? What should be the ideal coverage of the labeling functions?
    \item What if higher coverage leads to higher False Positives?
    \item Labeling function weights for the functions with high F1 score (and not a good coverage necessarily)?
    \item How is overlap calculated? are only positive classes calculated in the overlap? > Coverage is calculated over 0 and 1 labels excluding the -1 labels.
    \item Is coverage correlated to F1 score?
    \item How is abstain treated while calculating F1 score?
    \item Check out Trove for how "0" labels are calculated?
    \item What annotation effort or how much annotation effort will be saved using distant-pico?
\end{enumerate}

Learning rate for label model train should be lower. LR like 1.0, 0.5, 0.1, 0.05, 0.01 did not work for me.

Problem of high precision and low recall. Could it solved by reducing the abstains?
%
%
%
%==============================
\bibliographystyle{vancouver}
\bibliography{literature}
%==============================

\end{document}